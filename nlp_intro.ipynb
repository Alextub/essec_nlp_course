{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Introduction to Natural Language Processing (NLP)\n",
    "\n",
    "---\n",
    "* RÃ©my Frenoy\n",
    "* https://www.linkedin.com/in/rfrenoy/\n",
    "* https://github.com/rfrenoy\n",
    "* https://github.com/rfrenoy/essec_nlp_course\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Disclaimer\n",
    "\n",
    "* Lot of content in a little time\n",
    "* Focus on main ideas, lot of links to best resources I know if you want to discover more.\n",
    "* All content is available, at the end I'll show you how to run the code on your laptops"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Why is NLP an important Topic?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# NLP and machine learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## What's machine learning?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Tom Mitchell: *\"A computer program is said to learn from experience E with respect to some class of tasks T and performance measure P if its performance at tasks in T, as measured by P, improves with experience E\"*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Experience E -> new data\n",
    "* Class of task T -> Classification, generation, translation, ...\n",
    "* Performance P -> How good the model is at doing T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The big difference between machine learning and \"classical\" programming is the fact that *we cannot explicitely program the rules* (it wouldn't evolve with experience *E*). So how do we find the rules?\n",
    "\n",
    "Instead of computing a result based on a set of predefined criteria, let's say \"if the flat is located in the 5th arrondissement of Paris, then its value per square meter is 13000 euros\", we will construct some *\"vector of criteria\"* and some *\"vector of weights\"*, and our output will be the inner product of the two.\n",
    "\n",
    "We will **optimize** the choice of values for our vector of weights to *minimize some cost function*, generally using [gradient descent](https://youtu.be/AeRwohPuUHQ)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## An example of regression\n",
    "\n",
    "|    |   arrondissement |   floor |   max floor in building |   price per meter square |\n",
    "|---:|-----------------:|--------:|------------------------:|-------------------------:|\n",
    "|  0 |               17 |       7 |                       7 |                    11000 |\n",
    "|  1 |                5 |       1 |                       7 |                    10000 |\n",
    "|  2 |                3 |       4 |                       4 |                    13000 |\n",
    "|  3 |               20 |       2 |                       3 |                     9500 |\n",
    "|  4 |               19 |      11 |                      20 |                     8700 |\n",
    "|  5 |               17 |       1 |                       6 |                     8000 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Our model will try to find the price from the three first kinds of information. We can express our loss as some kind of difference between the predicted price and the real price, choose the loss function wisely so as to be able to compute gradient descent on it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## What about now?\n",
    "\n",
    "|    |   id | text                                                                                                                                  |   is real disaster? |\n",
    "|---:|-----:|:--------------------------------------------------------------------------------------------------------------------------------------|--------------------:|\n",
    "|  0 |    1 | Our Deeds are the Reason of this #earthquake May ALLAH Forgive us all                                                                 |                   1 |\n",
    "|  1 |    4 | Forest fire near La Ronge Sask. Canada                                                                                                |                   1 |\n",
    "|  2 |    5 | All residents asked to 'shelter in place' are being notified by officers. No other evacuation or shelter in place orders are expected |                   1 |\n",
    "|  3 |    6 | 13,000 people receive #wildfires evacuation orders in California                                                                      |                   1 |\n",
    "|  4 |    7 | Just got sent this photo from Ruby #Alaska as smoke from #wildfires pours into a school                                               |                   1 |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "NLP may be the domain where data preparation has the biggest impact on performance. Choices made during this phase can make the difference between a bad, a good and an excellent model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**Choices made during data preparation depends on the context studied.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "*Example: It is usually good practice to standardize the text and put everything in lower or upper case. But if you are trying to classify toxic comments, case carry signal, and this phase of standardization will lower your final model performances.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Extracting data and format it with regular expression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "fake_operation_data = pd.read_excel('fake_operation_data.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "print(fake_operation_data.to_markdown())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "def sanitize_comments(raw_df):\n",
    "    import re\n",
    "    new_lines = []\n",
    "    for i, row in raw_df.iterrows():\n",
    "        # p = re.compile(r\"'?\\-? ?([\\w | |'|,]*)\", re.IGNORECASE)\n",
    "        p = re.compile(r\"'?\\-? ?(.*)\", re.IGNORECASE)\n",
    "        new_line_comments = [match for match in p.findall(row.comment) \n",
    "                             if match != '']\n",
    "        new_lines.append(pd.DataFrame({'timestamp': [row.timestamp] * len(new_line_comments),\n",
    "                                       'comment': new_line_comments}))\n",
    "    return pd.concat(new_lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "cleaned_df = sanitize_comments(fake_operation_data)\n",
    "print(cleaned_df.to_markdown())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "A lot of tools exist for you to quickly test your regular expressions, such as https://regex101.com/. The library documentation is available on https://docs.python.org/3/howto/regex.html."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Tokenisation, lemmatization, stop words, ...\n",
    "\n",
    "There are a lot of words in a vocabulary, some carrying more meaning than others. The goal of tokenisation is to separate sentences into terms. Lemmatization tries to standardize family of words to their common root to reduce the number of dimensionalities (= making the problem simpler). Stop words remove the words that appear so often that they should not carry differentiating meaning.\n",
    "\n",
    "**Treat this with care!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "text = cleaned_df.comment.iloc[3]\n",
    "print(text)\n",
    "print('---')\n",
    "doc = nlp(text)\n",
    "for token in doc:\n",
    "    print(token)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "text = cleaned_df.comment.iloc[3]\n",
    "print(\"{:>10s}\\t{:>10s}\\t{:>10s}\".format('Token', 'Lemma', 'Stopword'))\n",
    "print(\"-\"*50)\n",
    "doc = nlp(text)\n",
    "for token in doc:\n",
    "    print(\"{:>10s}\\t{:>10s}\\t{:>10s}\".format(str(token), str(token.lemma_), str(token.is_stop)))\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Emojis\n",
    "\n",
    "A good example showing that language is a continuously-evolving domain, the use of emojis is now very common. So common that taking into account the meaning of emojis can have a tremendous impact on your model! (It depends on your context again... If you work for Doctrine, you should not see emojis that often ;-) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import re\n",
    "\n",
    "r = requests.get('https://unicode.org/Public/emoji/12.0/emoji-test.txt')\n",
    "lines = r.text.split('\\n')\n",
    "filtered_lines = [line for line in lines if len(line) > 0 and line[0] != '#']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "filtered_lines[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "p = re.compile(\".*# ([^\\s])* (.*)\")\n",
    "code, meaning = zip(*[p.findall(line)[0] for line in filtered_lines])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "my_emoji_translator = {c: meaning[i] for i,c in enumerate(code)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "my_emoji_translator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "text = cleaned_df.comment.iloc[3]\n",
    "print(\"{:>10s}\\t{:>10s}\\t{:>10s}\".format('Token', 'Lemma', 'Stopword'))\n",
    "print(\"-\"*50)\n",
    "doc = nlp(text)\n",
    "for token in doc:\n",
    "    if str(token) in my_emoji_translator:\n",
    "        emoji_translation = my_emoji_translator[str(token)]\n",
    "        doc_for_emoji = nlp(emoji_translation)\n",
    "        for t in doc_for_emoji:\n",
    "            print(\"{:>10s}\\t{:>10s}\\t{:>10s}\".format(str(t), str(t.lemma_), str(t.is_stop)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Diaporama",
  "kernelspec": {
   "display_name": "essec_course_env",
   "language": "python",
   "name": "essec_course_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
